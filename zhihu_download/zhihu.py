import requests
import os
import time
import re
import datetime
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from urllib.parse import urljoin

# ====================== ã€ä»…éœ€ä¿®æ”¹è¿™3ä¸ªåœ°æ–¹ â†“â†“â†“ã€‘ ======================
USER_ID = "mr-dang-77"       # æ›¿æ¢æˆç›®æ ‡çŸ¥ä¹åšä¸»çš„user_id
AUTHOR_NAME = "MR Dang"      # æ›¿æ¢æˆåšä¸»æ˜µç§°ï¼Œç”¨äºåˆ›å»ºæœ¬åœ°ä¿å­˜æ–‡ä»¶å¤¹
MIN_ANSWER_DATE = "2025-01-01"  # ä»…ä¸‹è½½æ­¤æ—¥æœŸåŠä¹‹åçš„å›ç­”ï¼Œæ ¼å¼ï¼šYYYY-MM-DD
# =======================================================================

# åˆå§‹åŒ–è¯·æ±‚å¤´ - é˜²åçˆ¬æ ¸å¿ƒé…ç½®
# ua = UserAgent()  # æš‚æ—¶ä¸ä½¿ç”¨éšæœºUAï¼Œä½¿ç”¨å›ºå®šUAæ›´ç¨³å®š
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Referer": "https://www.zhihu.com/",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    # ã€å…³é”®ã€‘ï¼šè¯·åœ¨æµè§ˆå™¨ç™»å½•çŸ¥ä¹ï¼ŒæŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…· -> Network -> åˆ·æ–°é¡µé¢ -> æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¯·æ±‚ -> å¤åˆ¶Request Headersä¸­çš„Cookieå€¼å¡«å…¥ä¸‹æ–¹
    "Cookie": "SESSIONID=iCoussn7NugIg29g6jEwWtCL8PgJCqqaF5TF3KQKqGm; JOID=W1wRB0zqc_wynXtfEYrzYtB8mUkHoQqaSdExLiPWE5d07zwoR_vnl1aedl8S0pzEHVam-GO332WGXwvV71VNvg0=; osd=WlgXC07rd_o-n3pbF4bxY9R6lUsGpQyWS9A1KC_UEpNy4z4pQ_3rlVeacFMQ05jCEVSn_GW73WSCWQfX7lFLsg8=; _xsrf=jEXlaCMbkp6OlTH1i5LD9kYkWNYyDsF0; _zap=db664e23-8200-4346-ac03-7784159d83f5; d_c0=FweUWuHvFRuPTvK5JqO6H92GpsyyU2BtZAw=|1758108637; HMACCOUNT=B4F69BF209DB9E9E; DATE=1758206982575; crystal=U2FsdGVkX1+l2Me1mx5fVEe94CGPCnELMfntmaPXv5XQjsolGZwSpaBzBYMDdvvOdXW0vd2NYlY8pMyUoD0tCNPecyzqRdpYgwm/o1KQBfFCl64+aZgZUG+RVJPcftuU7Yx73O6V/Ga6O/iDMTjrBYQ2JQS+RS33GIDHTR/hwZtS8V5rfRB600u4+3pfP0NEih/yC2ozl3S8GLnPmfSX4soHu0VvJUWGmuParNzXXm7dJ0z8cEo3ahL71R10YrmU; vmce9xdq=U2FsdGVkX19GjFQBruXHIxbFk0ZkASlGs3NMM9AznZ9EoD1X1zrGuY1VMpcaYwSGN82p0js35oy782x67igFPF+BXPvC2odwalA9EThp1s+iugIx4Vz+8EUbHwbF8YAg+S29h+hjDGeI3SfirOCn+5FLzyP6X6J8hnEx0Ce4WZ4=; Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49=1765892192; Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49=1765971228; __snaker__id=3X2uXvk4oUWGxcaj; gdxidpyhxdE=yZVUb8At%2ByEL7qocoLk6OyLPOdLH9cvipLuMtTOmDZchgZtJs%2ByPBvYD%5C8rewEH1E9vDxLcZTH%2FEfaVnPyz9omk6gxUr0sYMh%2BrjxZ2q6sLLrxQ6Bl9yi%5CC9lnmdi9A9yxtDBmokPSSeHbuLpURYanlB%5Ct0Awnsc8bw7wnqQIhh0DMV4%3A1767185225832; captcha_session_v2=2|1:0|10:1767184339|18:captcha_session_v2|88:SnhsRS9WU2lZdnBycWJubzJxNzM3aWNpbWtDUFBxU2U2aVZvLzRFRVhuc1dZRE1YYU5BNWRDV2ZQMEhxZVBsbQ==|6ce458b266ba3d33889ddf3497d7c43bcad3317434e749b20179f24a85dfca6f; cmci9xde=U2FsdGVkX1/KaUewWQp9LX2TECa09SggRMvNquf86WlZBcMohfk1fIH1xZ9mdT9jmJAggwphYCyGAqoI2u7gFg==; pmck9xge=U2FsdGVkX1+0xi4no6I9vhKtqqPUJFHmRFtMcU7Hrps=; assva6=U2FsdGVkX19MkhvUZRVeEUTHCLt6Lg2nM45kvjr8WxY=; assva5=U2FsdGVkX19nE+eYR8AaBz5Ks9ITJYBTgctKsKMqbdc5tJe0gqFhzyL0fi3dA1X4pLihw3O7zSFM7hdleDKquA==; captcha_ticket_v2=2|1:0|10:1767184355|17:captcha_ticket_v2|728:eyJ2YWxpZGF0ZSI6IkNOMzFfT0dBb0xMVEwzOWFWVWhrbGExb2NESXRYcFNJS2hXTF9Da0N2MDRNLlJmbmlGM2REQnRIaExNNHJXMFFyKkNwTE5XbFBtalJUSENVU0t5eGZpbmFXKkZnNFAuVUNtelJHV012SzEqT1VPelI1OFBvR0lQR2FTNm1mSDBxZEU1TG5PSUdHVzZPVUFLNlJxOVByYWg0TVBGZFc4cGhzZUhndm9XdExpTzVnM1RRM0Zod0Z0OWNhWWhqWmJtbDE0Qypja0gua2NwNip4Wm1XVTBCazlMMnR3M09ONV9ZdDB0dk5UVUlnb3dMWVBtU3VkTW9GU3pLbkNTb3g5eWNKcTNSSWUqbjFnVXguYl9FZVRNd25hUlhKdlkwTlpYMW5YUXJicVFDZjNndVRZaEFSUk4xKkhUR3R6Y3ltKmNmdnVsbmFXMTFXeVdJcXB5ZVR5YW1mYl9Wc1pEb0Jua2VDOU14czZqaGFib3MzWXVFWGFmeHVuZDZKUWtNZHQxeE1YZFBsNlpxRnZxNipVNGwzTEhLU0g0RHdxQnduckVhanhUM0RjTWVERS5KOFNzUmpOb3VZMEZ0bnhpemc1bG1jdHk0NEdDZDE1TU9BMnRJNXFicmRCRFZfbGQ1U3BiTXBseDJneVVOcDU2SjBUNXFVODhBdEdWaldka0F4cmRHenNHRzZRY3Ztamc3N192X2lfMSJ9|d33a0cb643a7416baebd976ac730f96e642bed93d76171a5ad3a08b61a80ed1f; z_c0=2|1:0|10:1767200920|4:z_c0|92:Mi4xWURvUUFBQUFBQUFYQjVSYTRlOFZHeVlBQUFCZ0FsVk44V1ZDYWdCRDZHaHVkTnN1b0RPRFB1WGdxcTZYMkgzVG1B|7548f61fc02feaacd33e13d0f70a255dfb4ac7c1c56c02783dcf0bab49c74af2; q_c1=5e01cb87547449eebbc4590a090268cc|1767282754000|1767282754000; SESSIONID=FyfxBGq22mkFqIzVuzODQle23oSRcYP5z5dw0QeCF8A; JOID=V1gQA02oSVELDfzWWMDMwunsFsVD6zUydUy-om-ZIT9IerapCxnIMWcL_NZbIll0rTYAQ4Regw7EwCB8tZsX6Q8=; osd=U1sSAU-sSlMJD_jVWsLOxuruFMdH6Dcwd0i9oG2bJTxKeLStCBvKM2MI_tRZJlp2rzQEQIZcgQrHwiJ-sZgV6w0=; __zse_ck=005_tt0mOpAxNR8PncOPe31NfdVNaAuRipijioE0w4D5MfKUBpyz5LPJuQlHFD=7CcbVY=e3O2bkwtKosB1fEheHdN3L4JuoWmwJji/b6/YV6ZJsGtE588dhMRD1FBWGAvCu-1y43Nwo1f2Sd8gQ/6ujcAavUCiuwnJbaeDDjwuq4KvarW3OriAc54Bg7sCAaKCKH8CfjA+OndIIsQLdEInd8afQt+nD02UiNNtiJhzpLGgig9JM8fsuice5zz1SGZ/U6; BEC=f7bc18b707cd87fca0d61511d015686f"}

# åˆ›å»ºä¸»ä¿å­˜æ–‡ä»¶å¤¹
main_save_dir = f"çŸ¥ä¹_{AUTHOR_NAME}_å›ç­”åˆé›†(å«æœ¬åœ°å›¾ç‰‡)"
if not os.path.exists(main_save_dir):
    os.makedirs(main_save_dir)

def clean_file_name(title):
    """æ¸…æ´—æ ‡é¢˜/æ–‡ä»¶åï¼Œå»é™¤ç³»ç»Ÿéæ³•å­—ç¬¦ï¼Œé˜²æ­¢ä¿å­˜å¤±è´¥"""
    illegal_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|', 'ï¼Œ', 'ã€‚']
    for char in illegal_chars:
        title = title.replace(char, "")
    # å»é™¤é¦–å°¾ç©ºæ ¼å’Œæ¢è¡Œ
    return title.strip()[:50]  # æ ‡é¢˜è¿‡é•¿æˆªå–å‰50å­—ç¬¦ï¼Œé¿å…æ–‡ä»¶åè¿‡é•¿

def download_img_and_replace_md_link(md_content, article_title, article_url, date_str):
    """
    æ ¸å¿ƒåŠŸèƒ½å‡½æ•°ï¼šä¸‹è½½æ–‡ç« å†…æ‰€æœ‰å›¾ç‰‡åˆ°æœ¬åœ° + æ›¿æ¢markdownä¸­çš„ç½‘ç»œå›¾ç‰‡é“¾æ¥ä¸ºæœ¬åœ°è·¯å¾„
    :param md_content: åŸå§‹markdownæ–‡æœ¬å†…å®¹
    :param article_title: æ–‡ç« æ ‡é¢˜ï¼Œç”¨äºåˆ›å»ºä¸“å±å›¾ç‰‡æ–‡ä»¶å¤¹
    :param article_url: æ–‡ç« é“¾æ¥ï¼Œç”¨äºRefereré˜²ç›—é“¾
    :return: æ›¿æ¢æœ¬åœ°è·¯å¾„åçš„markdownå†…å®¹
    """
    img_sub_dir = f"{date_str}_{clean_file_name(article_title)}_å›¾ç‰‡"
    img_save_path = os.path.join(main_save_dir, img_sub_dir)
    if not os.path.exists(img_save_path):
        os.makedirs(img_save_path)

    # æ­£åˆ™åŒ¹é…markdownä¸­çš„æ‰€æœ‰å›¾ç‰‡é“¾æ¥ï¼š![å›¾ç‰‡æè¿°](å›¾ç‰‡URL)
    img_pattern = re.compile(r"!\[(.*?)\]\((https?://.*?)\)")
    all_img = img_pattern.findall(md_content)

    if not all_img:
        return md_content  # æ— å›¾ç‰‡åˆ™ç›´æ¥è¿”å›åŸå†…å®¹

    # éå†æ‰€æœ‰å›¾ç‰‡ï¼Œä¸‹è½½+æ›¿æ¢é“¾æ¥
    for img_desc, img_url in all_img:
        try:
            # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åï¼Œé˜²æ­¢é‡å¤/éæ³•å­—ç¬¦
            img_suffix = img_url.split(".")[-1].lower()
            if img_suffix not in ["jpg", "png", "gif", "webp", "jpeg"]:
                img_suffix = "jpg"
            img_name = f"{clean_file_name(img_desc)}_{int(time.time())}.{img_suffix}"
            img_file_path = os.path.join(img_save_path, img_name)

            # å›¾ç‰‡å·²å­˜åœ¨åˆ™è·³è¿‡ä¸‹è½½ï¼Œé¿å…é‡å¤è¯·æ±‚
            if not os.path.exists(img_file_path):
                # å…³é”®ï¼šè®¾ç½®Refererä¸ºæ–‡ç« é“¾æ¥ï¼Œè§£å†³403 Forbidden
                img_headers = headers.copy()
                img_headers["Referer"] = article_url
                
                img_response = requests.get(img_url, headers=img_headers, timeout=10)
                img_response.raise_for_status()
                # äºŒè¿›åˆ¶å†™å…¥å›¾ç‰‡æ–‡ä»¶
                with open(img_file_path, "wb") as f:
                    f.write(img_response.content)
                time.sleep(0.2)  # å›¾ç‰‡ä¸‹è½½é—´éš”ï¼Œé˜²åçˆ¬

            # å…³é”®ï¼šå°†markdownä¸­çš„ã€ç½‘ç»œURLã€‘æ›¿æ¢ä¸ºã€æœ¬åœ°ç›¸å¯¹è·¯å¾„ã€‘ï¼Œä¿è¯æ‰“å¼€mdèƒ½ç›´æ¥åŠ è½½å›¾ç‰‡
            md_content = md_content.replace(img_url, img_sub_dir + "/" + img_name)
        except Exception as e:
            print(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_url} | åŸå› : {str(e)[:20]}")
            continue
    return md_content

def get_zhihu_author_all_answers(user_id, min_answer_date_str=None):
    """è·å–çŸ¥ä¹åšä¸»çš„å…¨éƒ¨å›ç­”åˆ—è¡¨ï¼ˆæ ‡é¢˜+é“¾æ¥+æ—¥æœŸï¼‰ï¼Œåˆ†é¡µåŠ è½½æ‰€æœ‰å†…å®¹"""
    answer_list = []

    min_ts = None
    if min_answer_date_str:
        try:
            min_dt = datetime.datetime.strptime(min_answer_date_str, "%Y-%m-%d")
            min_ts = int(min_dt.timestamp())
        except Exception:
            min_ts = None

    offset = 0
    limit = 20
    answer_api = f"https://www.zhihu.com/api/v4/members/{user_id}/answers"
    print("\nğŸ” æ­£åœ¨è·å–åšä¸»çš„æ‰€æœ‰ã€å›ç­”ã€‘åˆ—è¡¨...")
    while True:
        try:
            params = {
                "include": "data[*].id,question.title,url,created_time,updated_time",
                "offset": offset,
                "limit": limit,
                "sort_by": "created"
            }
            res = requests.get(answer_api, headers=headers, params=params, timeout=10)
            res.raise_for_status()
            data = res.json()

            if not data.get("data"):
                break

            for item in data["data"]:
                question_title = item.get("question", {}).get("title", "æ— æ ‡é¢˜å›ç­”")
                title = clean_file_name(question_title)
                url = f"https://www.zhihu.com/question/{item['question']['id']}/answer/{item['id']}"
                created_ts = item.get("created_time")
                if min_ts and created_ts and created_ts < min_ts:
                    continue
                date_str = datetime.datetime.fromtimestamp(created_ts).strftime('%Y-%m-%d') if created_ts else "0000-00-00"
                answer_list.append((title, url, date_str))

            print(f"   >> å·²åŠ è½½ {len(data['data'])} ä¸ªå›ç­”ï¼Œç»§ç»­åŠ è½½ä¸‹ä¸€é¡µ...")
            if data["paging"]["is_end"]:
                break

            offset += limit
            time.sleep(1.5)
        except Exception as e:
            print(f"âš ï¸ è·å–å›ç­”åˆ—è¡¨å¼‚å¸¸: {str(e)}")
            break
            
    return answer_list

def parse_zhihu_article_to_markdown(article_url):
    """è§£æå•ç¯‡çŸ¥ä¹æ–‡ç« ï¼Œæå–æ­£æ–‡å¹¶è½¬ä¸ºæ ‡å‡†Markdownæ ¼å¼"""
    try:
        res = requests.get(article_url, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        # çŸ¥ä¹æ–‡ç« æ­£æ–‡çš„å›ºå®šæ ¸å¿ƒå®¹å™¨ï¼Œæ‰€æœ‰å†…å®¹éƒ½åœ¨è¿™é‡Œ
        content_box = soup.find("div", class_="RichContent-inner")
        if not content_box:
            return "ã€âš ï¸ è¯¥æ–‡ç« å·²è¢«åˆ é™¤/æƒé™ä¸è¶³ï¼Œæ— æ³•æŸ¥çœ‹å†…å®¹ã€‘"
        
        # HTMLè½¬Markdownï¼Œå®Œç¾ä¿ç•™çŸ¥ä¹æ’ç‰ˆæ ¼å¼
        markdown_content = md(str(content_box), heading_style="ATX")
        # æ¸…ç†å¤šä½™ç©ºè¡Œï¼Œè®©markdownæ›´æ•´æ´
        markdown_content = "\n".join([line for line in markdown_content.split("\n") if line.strip()])
        return markdown_content
    except Exception as e:
        return f"ã€âš ï¸ æ–‡ç« è§£æå¤±è´¥ã€‘é”™è¯¯åŸå› : {str(e)[:30]}"

def save_markdown_file(article_title, markdown_content):
    """å°†å¤„ç†å¥½çš„markdownå†…å®¹ï¼ˆå«æœ¬åœ°å›¾ç‰‡é“¾æ¥ï¼‰ä¿å­˜ä¸º.mdæ–‡ä»¶"""
    md_file_name = f"{article_title}.md"
    md_file_path = os.path.join(main_save_dir, md_file_name)
    # å»é‡ï¼šæ–‡ä»¶å·²å­˜åœ¨åˆ™è·³è¿‡ï¼Œé¿å…é‡å¤ä¸‹è½½
    if os.path.exists(md_file_path):
        print(f"âœ… å·²å­˜åœ¨ï¼Œè·³è¿‡ï¼š{md_file_name}")
        return
    # å†™å…¥æ–‡ä»¶ï¼ŒæŒ‡å®šutf-8ç¼–ç é˜²æ­¢ä¸­æ–‡ä¹±ç 
    with open(md_file_path, "w", encoding="utf-8") as f:
        f.write(f"# {article_title}\n\n")  # æ ‡é¢˜ä½œä¸ºä¸€çº§æ ‡é¢˜
        f.write(markdown_content)
    print(f"âœ… ä¿å­˜æˆåŠŸï¼š{md_file_name}")

if __name__ == "__main__":
    print("=" * 50)
    print(f"å¼€å§‹çˆ¬å–ã€{AUTHOR_NAME}ã€‘çš„çŸ¥ä¹å…¨éƒ¨å›ç­” + å›¾ç‰‡æœ¬åœ°åŒ–ä¸‹è½½")
    print("=" * 50)
    
    all_answers = get_zhihu_author_all_answers(USER_ID, MIN_ANSWER_DATE)
    
    if not all_answers:
        print("âŒ æœªè·å–åˆ°ä»»ä½•å›ç­”ï¼Œè¯·æ£€æŸ¥ã€user_idã€‘æ˜¯å¦æ­£ç¡®ï¼")
        print("ğŸ’¡ æç¤ºï¼šå¦‚æœé‡åˆ°401/403é”™è¯¯ï¼Œè¯·åŠ¡å¿…æ›´æ–°ä»£ç ä¸­çš„ã€Cookieã€‘ï¼")
    else:
        total = len(all_answers)
        print(f"\nğŸ‰ å…±è·å–åˆ° {total} æ¡å›ç­”ï¼Œå¼€å§‹è§£æ+ä¸‹è½½å›¾ç‰‡+ä¿å­˜...\n")
        
        for index, (title, url, date_str) in enumerate(all_answers, start=1):
            print(f"\n[{index}/{total}] [{date_str}] æ­£åœ¨å¤„ç†ï¼š{title}")
            
            raw_md = parse_zhihu_article_to_markdown(url)
            
            final_md = download_img_and_replace_md_link(raw_md, title, url, date_str)
            
            file_name_prefix = f"{date_str}_{title}"
            save_markdown_file(file_name_prefix, final_md)
            
            time.sleep(1)  # é—´éš”ï¼Œé˜²åçˆ¬
    
    print("\n" + "=" * 50)
    print("âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼æ‰€æœ‰æ–‡ç« å’Œå›¾ç‰‡å·²ä¿å­˜è‡³ï¼š", main_save_dir)
    print("=" * 50)
